{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce4d2b3-89ff-456f-988f-2834945f5777",
   "metadata": {},
   "source": [
    "<h1 style=\"color: navy; text-align: center;\">Credit Risk Model Exploratory Data Analysis</h1>\n",
    "<p style=\"text-align: justify; font-size: 16px;\">\n",
    "This notebook delves into the dataset's data exploration, providing insights crucial for evaluating the default risk of potential clients. By enabling consumer finance providers to approve a higher number of loan applications, this analysis contributes to improving the financial inclusiveness of individuals previously excluded due to insufficient credit history.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cff257e-b1de-4174-809c-2c5e5e8da09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Oreos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_palette(\"colorblind\") \n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "script_dir = os.path.dirname(os.path.abspath('PreprocessingSteps.ipynb'))\n",
    "parent_directory = os.path.dirname(script_dir)\n",
    "module_directory = os.path.join(parent_directory, 'module') \n",
    "utils_directory = os.path.join(parent_directory, 'utils') \n",
    "\n",
    "if (parent_directory not in sys.path):\n",
    "    sys.path.append(parent_directory)\n",
    "    \n",
    "if (module_directory not in sys.path):\n",
    "    sys.path.append(module_directory)\n",
    "    \n",
    "if (utils_directory not in sys.path):\n",
    "    sys.path.append(utils_directory)    \n",
    "\n",
    "# created files\n",
    "from utils import config\n",
    "from module.preprocess.bpe import BpeArgs, Encoder\n",
    "from module.preprocess.load_and_batch import DataBatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78824ff-8ca3-4847-9bf9-637c7f551cc8",
   "metadata": {},
   "source": [
    "<h1 style=\"color: navy; font-family: Verdana, Geneva, sans-serif;\">Exploring Information for Depth 0</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: 'Lucida Grande', 'Lucida Sans Unicode', Arial, sans-serif; color: #333;\">\n",
    "  At depth zero, we have <strong style=\"color: darkred;\">static features</strong> tied to a specific credit case. All the features here can be directly used as predictors.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edeb956-9f11-4fd2-8d3b-abcaf3bcbcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------> Size of train dataset: (1526659, 224) <------\n",
      "------> Size of categorical columns: 185 <------\n",
      "------> Size of numeric columns: 35 <------\n"
     ]
    }
   ],
   "source": [
    "preprocess = DataBatcher()\n",
    "preprocess.load_and_process(config.DATA_LOCATION, training=True, train_test_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86102e7-2433-4096-9a82-2f20c4bdd2db",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/confused.gif\" 00\" alt=\"Confused\">\n",
    "    <p style=\"text-align: center; font-style: italic; font-weight: bold;\">Just the 224 Columns Then.........</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc590898-cb11-41b2-86c0-189d8faa55cb",
   "metadata": {},
   "source": [
    "## Training Data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a264784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------> Shape of training data: (1221327, 222) <---------------\n",
      "---------------> Shape of test data: (305332, 222) <---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 222)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>case_id</th><th>actualdpdtolerance_344P</th><th>amtinstpaidbefduel24m_4187115A</th><th>annuity_780A</th><th>annuitynextmonth_57A</th><th>applicationcnt_361L</th><th>applications30d_658L</th><th>applicationscnt_1086L</th><th>applicationscnt_464L</th><th>applicationscnt_629L</th><th>applicationscnt_867L</th><th>avgdbddpdlast24m_3658932P</th><th>avgdbddpdlast3m_4187120P</th><th>avgdbdtollast24m_4525197P</th><th>avgdpdtolclosure24_3658938P</th><th>avginstallast24m_3658937A</th><th>avglnamtstart24m_4525187A</th><th>avgmaxdpdlast9m_3716943P</th><th>avgoutstandbalancel6m_4187114A</th><th>avgpmtlast12m_4525200A</th><th>bankacctype_710L</th><th>cardtype_51L</th><th>clientscnt12m_3712952L</th><th>clientscnt3m_3712950L</th><th>clientscnt6m_3712949L</th><th>clientscnt_100L</th><th>clientscnt_1022L</th><th>clientscnt_1071L</th><th>clientscnt_1130L</th><th>clientscnt_136L</th><th>clientscnt_157L</th><th>clientscnt_257L</th><th>clientscnt_304L</th><th>clientscnt_360L</th><th>clientscnt_493L</th><th>clientscnt_533L</th><th>clientscnt_887L</th><th>&hellip;</th><th>for3years_504L</th><th>for3years_584L</th><th>formonth_118L</th><th>formonth_206L</th><th>formonth_535L</th><th>forquarter_1017L</th><th>forquarter_462L</th><th>forquarter_634L</th><th>fortoday_1092L</th><th>forweek_1077L</th><th>forweek_528L</th><th>forweek_601L</th><th>foryear_618L</th><th>foryear_818L</th><th>foryear_850L</th><th>fourthquarter_440L</th><th>maritalst_385M</th><th>maritalst_893M</th><th>numberofqueries_373L</th><th>pmtaverage_3A</th><th>pmtaverage_4527227A</th><th>pmtaverage_4955615A</th><th>pmtcount_4527229L</th><th>pmtcount_4955617L</th><th>pmtcount_693L</th><th>pmtscount_423L</th><th>pmtssum_45A</th><th>requesttype_4525192L</th><th>responsedate_1012D</th><th>responsedate_4527233D</th><th>responsedate_4917613D</th><th>riskassesment_302T</th><th>riskassesment_940T</th><th>secondquarter_766L</th><th>thirdquarter_1082L</th><th>date_decision</th><th>target</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>date</td><td>date</td><td>date</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>date</td><td>i64</td></tr></thead><tbody><tr><td>106054</td><td>0.0</td><td>null</td><td>4051.0</td><td>5785.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>5.0</td><td>-8.0</td><td>null</td><td>null</td><td>0.0</td><td>5202.8003</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>&quot;CA&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>1.0</td><td>10956.967</td><td>null</td><td>null</td><td>null</td><td>null</td><td>6.0</td><td>null</td><td>null</td><td>null</td><td>2019-02-04</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2.0</td><td>0.0</td><td>2019-01-21</td><td>0</td></tr><tr><td>1875612</td><td>0.0</td><td>0.0</td><td>6863.8003</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;INSTANT&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>5.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>4.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2020-07-17</td><td>null</td><td>null</td><td>1.0</td><td>4.0</td><td>2020-07-03</td><td>0</td></tr><tr><td>1523012</td><td>0.0</td><td>75811.68</td><td>3333.4001</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-17.0</td><td>null</td><td>-17.0</td><td>0.0</td><td>6317.6</td><td>null</td><td>0.0</td><td>null</td><td>8372.8</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>3.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>6.0</td><td>50112.6</td><td>&quot;DEDUCTION_6&quot;</td><td>2019-09-20</td><td>2019-09-20</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>1.0</td><td>2019-09-06</td><td>0</td></tr><tr><td>628698</td><td>null</td><td>null</td><td>2979.4001</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;CA&quot;</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>4.0</td><td>&quot;a55475b1&quot;</td><td>&quot;a55475b1&quot;</td><td>7.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>3.0</td><td>5.0</td><td>2019-02-09</td><td>0</td></tr><tr><td>861635</td><td>0.0</td><td>5417.6</td><td>13503.0</td><td>1791.8</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>4.0</td><td>-2.0</td><td>-3.0</td><td>-3.0</td><td>0.0</td><td>1805.8</td><td>17051.6</td><td>0.0</td><td>18616.6</td><td>1805.8</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>null</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>&hellip;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11.0</td><td>&quot;3439d993&quot;</td><td>&quot;a55475b1&quot;</td><td>16.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;DEDUCTION_6&quot;</td><td>null</td><td>2019-11-29</td><td>null</td><td>null</td><td>null</td><td>3.0</td><td>6.0</td><td>2019-11-15</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 222)\n",
       "┌─────────┬────────────┬────────────┬────────────┬───┬────────────┬───────────┬───────────┬────────┐\n",
       "│ case_id ┆ actualdpdt ┆ amtinstpai ┆ annuity_78 ┆ … ┆ secondquar ┆ thirdquar ┆ date_deci ┆ target │\n",
       "│ ---     ┆ olerance_3 ┆ dbefduel24 ┆ 0A         ┆   ┆ ter_766L   ┆ ter_1082L ┆ sion      ┆ ---    │\n",
       "│ i64     ┆ 44P        ┆ m_4187115A ┆ ---        ┆   ┆ ---        ┆ ---       ┆ ---       ┆ i64    │\n",
       "│         ┆ ---        ┆ ---        ┆ f64        ┆   ┆ f64        ┆ f64       ┆ date      ┆        │\n",
       "│         ┆ f64        ┆ f64        ┆            ┆   ┆            ┆           ┆           ┆        │\n",
       "╞═════════╪════════════╪════════════╪════════════╪═══╪════════════╪═══════════╪═══════════╪════════╡\n",
       "│ 106054  ┆ 0.0        ┆ null       ┆ 4051.0     ┆ … ┆ 2.0        ┆ 0.0       ┆ 2019-01-2 ┆ 0      │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆           ┆ 1         ┆        │\n",
       "│ 1875612 ┆ 0.0        ┆ 0.0        ┆ 6863.8003  ┆ … ┆ 1.0        ┆ 4.0       ┆ 2020-07-0 ┆ 0      │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆           ┆ 3         ┆        │\n",
       "│ 1523012 ┆ 0.0        ┆ 75811.68   ┆ 3333.4001  ┆ … ┆ 0.0        ┆ 1.0       ┆ 2019-09-0 ┆ 0      │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆           ┆ 6         ┆        │\n",
       "│ 628698  ┆ null       ┆ null       ┆ 2979.4001  ┆ … ┆ 3.0        ┆ 5.0       ┆ 2019-02-0 ┆ 0      │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆           ┆ 9         ┆        │\n",
       "│ 861635  ┆ 0.0        ┆ 5417.6     ┆ 13503.0    ┆ … ┆ 3.0        ┆ 6.0       ┆ 2019-11-1 ┆ 0      │\n",
       "│         ┆            ┆            ┆            ┆   ┆            ┆           ┆ 5         ┆        │\n",
       "└─────────┴────────────┴────────────┴────────────┴───┴────────────┴───────────┴───────────┴────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"---------------> Shape of training data: {preprocess.train.data.shape} <---------------\")\n",
    "print(f\"---------------> Shape of test data: {preprocess.test.data.shape} <---------------\")\n",
    "display(preprocess.train.data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f44a5",
   "metadata": {},
   "source": [
    "# Exploring Data Conversion\n",
    "\n",
    "## Overview\n",
    "In this project, we explore the conversion of tabular data into text, focusing on representing numerical data as continuous variables. Notably, we dropped the *Week number* and *Month* information, relying instead on the 'date_decision' column for temporal insights. This strate**gy aims for a 1-to-1 mapp**ing between text and encoded numerical values, minimizing data scaling.\n",
    "\n",
    "## Converting Tabular Information to Text Tokens\n",
    "\n",
    "### Rationale\n",
    "The conversion process leverages neural network capabilities to process and interpret tabular data as text. This approach enables the utilization of features like null values in a more meaningful way through embeddings. During data augmentation, masked values are identified, allowing the model to effectively 'ignore' them during training. By employing specialized embedding tables and Byte Pair Encoding (B**hope to** PE), we enhance the model's ability to generalize across unseen inputs.\n",
    "\n",
    "### Process and Benefits\n",
    "\n",
    "#### Tokenization and Embedding\n",
    "- **Contextual Encoding**: Categorical values are either encoded or marked as empty, and numeric data presence is flagged, providing context to the data.\n",
    "- **Adaptability to New Data**: The method ensures adaptability, allowing the model to perform well with new data, as long as it shares similarities with the training set.\n",
    "- **Efficient Representation**: By avoiding one-hot encoding, we use embeddings to create a compact and meaningful data representation, mitigating issues related to dimensionality and data sparsity.\n",
    "\n",
    "### Visual Representation\n",
    "A diagram will be included here to illustrate the data conversion process, showcasing how tabular data is transformed into text tokens and the subsequent embedding and processing steps.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/Flow_w.png\" style=\"width: 50%;\" alt=\"Algorithm Process\">\n",
    "    <p style=\"text-align: center; font-style: italic; font-weight: bold;\">Steps Flowchart</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175b5d2",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "In our program, we utilize **Byte Pair Encoding (BPE)** to convert text into tokens. BPE is a method initially devised for data compression that has been effectively adapted for tokenization in natural language processing (NLP).\n",
    "\n",
    "#### Overview of Byte Pair Encoding (BPE)\n",
    "\n",
    "BPE operates by iteratively merging the most frequent pair of bytes or characters in a sequence. This technique was first used for compressing data but has since gained prominence in NLP for its efficiency in tokenization.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/tok_types.png\" style=\"width: 50%;\" alt=\"Byte Pair Encoding visualization\">\n",
    "    <p style=\"text-align: center; font-style: italic; font-weight: bold;\">Ways to tokenize characters. We are using the subword approach</p>\n",
    "</div>\n",
    "\n",
    "#### Key Features of BPE\n",
    "\n",
    "- **Efficiency**: BPE addresses the vocabulary explosion problem by merging frequently occurring pairs, effectively reducing the vocabulary size without losing significant information.\n",
    "\n",
    "- **Subword Tokenization**: It breaks down words into smaller units (subwords or characters), aiding in the handling of out-of-vocabulary words and the morphological variations of words.\n",
    "\n",
    "#### Chosen Method\n",
    "\n",
    "To effectively manage the vocabulary size and ensure the model can generalize to unseen scenarios, we employ a hybrid approach that combines subword-level and word-level tokenization strategies.\n",
    "ions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa524c",
   "metadata": {},
   "source": [
    "### Encoding Numeric Data in Text as Continuous Variables\n",
    "\n",
    "Our goal is to encode numbers within text (e.g., amounts, general numbers, dates) as continuous variables to improve the model’s understanding of their context.\n",
    "\n",
    "#### Methodology\n",
    "- **Text Parsing**: Identify and categorize numbers in text (e.g., $8.99 as `Amount`, 13 as `Num`, 2020-19-10 as `Date`).\n",
    "- **Embedding Selection**: Use distinct embedding tables for each category to reflect their unique attributes:\n",
    "  - `Amount`: Embed digits and scale indicators (e.g., K, M).\n",
    "  - `Num`: Include digits, decimal points, and negative signs.\n",
    "  - `Date`: Embed digits and date separators (e.g., -).\n",
    "- **Encoding Process**:\n",
    "  - Tokenize numbers into components (e.g., 8.99 into ['8', '.', '9', '9']).\n",
    "  - Retrieve embeddings for each token, forming a matrix.\n",
    "  - Apply self-attention over this matrix to generate a cohesive vector representation.\n",
    "  \n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"images/SA_White.png\" style=\"width: 50%;\" alt=\"Attention Mechanism\">\n",
    "    <p style=\"text-align: center; font-style: italic; font-weight: bold;\">Self-Attention Mechanism</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bcdc9e",
   "metadata": {},
   "source": [
    "### Training the BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430ab1e",
   "metadata": {},
   "source": [
    "#### Pseudocode for Encoding Table Information\n",
    "\n",
    "For a batch size of 10 we do the following. Important to not we combine Byte pair encoding with rule based splitting [Hence the use of the regex]\n",
    "\n",
    "1. **Chunk Data Columns by 10**: Begin by chunking your data columns into segments of 10 for more manageable processing.\n",
    "\n",
    "    ```python\n",
    "    chunks = chunk_data(data_columns, chunk_size=10)\n",
    "    ```\n",
    "\n",
    "2. **Analyze Each Chunk**: For each chunk, sample entries to find common regex patterns, and use these patterns to facilitate further analysis.\n",
    "\n",
    "    ```python\n",
    "    for chunk in chunks:\n",
    "        sample = get_sample(chunk, sample_size=100)  # Sample 100 entries\n",
    "        regex_patterns = find_common_regex_patterns(sample)  # Identify patterns\n",
    "        split_texts = split_text_using_patterns(sample, regex_patterns)  # Split text\n",
    "    ```\n",
    "\n",
    "3. **Combine and Tokenize Text**: Combine all the split text from each chunk along with the found bi-grams. Then tokenize the combined text.\n",
    "\n",
    "    ```python\n",
    "    combined_text = combine_all_split_text(chunks)\n",
    "    tokens = tokenize(combined_text, combined_bi_grams)\n",
    "    ```\n",
    "\n",
    "4. **Apply Byte Pair Encoding (BPE)**: Use BPE on the tokens to refine the vocabulary size to the desired maximum.\n",
    "\n",
    "    ```python\n",
    "    final_vocab = byte_pair_encoding(tokens, max_vocab_size=desired_max_vocab_size)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a37e4cd",
   "metadata": {},
   "source": [
    "#### Python example with comments\n",
    "```python\n",
    "model_init = BpeArgs(\n",
    "        pattern=None,\n",
    "        target_context=28, # if designing for a particular context window apply here\n",
    "        adhoc_tokens=[\"PAD\"], # any addtional key tokens or tags\n",
    "        adhoc_words=preprocess.train_data.columns, # The table columns are taken as keywords and have their token number\n",
    "        store_loc=config.BASE_LOCATION # storage location\n",
    "    )\n",
    "    \n",
    "ops = Encoder(model_init)\n",
    "while True:\n",
    "    data_list = preprocess.get_meta_data(batch_size=10000, data_type=\"train\", ignore_list=[\"case_id\", \"target\"], verbose= False)\n",
    "    \n",
    "    if len(data_list) > 0:\n",
    "        ops.compress_files(data_list, save_or_update_every=100)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7142bab2",
   "metadata": {},
   "source": [
    "#### Simulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc271ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Information\n",
      "----> Size of New encoder: 352\n",
      "----> Size of Trained encoder: 1731\n"
     ]
    }
   ],
   "source": [
    "# For instantiating new Encoder Instance\n",
    "model_init = BpeArgs(\n",
    "        pattern=None,\n",
    "        target_context=28, # if designing for a particular context window apply here\n",
    "        adhoc_tokens=[\"PAD\"], # any addtional key tokens or tags\n",
    "        adhoc_words=preprocess.train.data.columns, # The table columns are taken as keywords and have their token number\n",
    "        store_loc=\"\" # storage location\n",
    "    )\n",
    "    \n",
    "    \n",
    "new_encoder = Encoder(model_init) # New version with basic information\n",
    "\n",
    "trained_encoder = Encoder(model_init) # can be None\n",
    "trained_encoder.load_state(config.BASE_LOCATION)\n",
    "\n",
    "print(\"Base Information\")\n",
    "print(f\"----> Size of New encoder: {new_encoder.vocab_size}\")\n",
    "print(f\"----> Size of Trained encoder: {trained_encoder.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed98f0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of initial text: 9405\n",
      "+-----------------------------------------------+\n",
      "|               Analysis Results                |\n",
      "+-----------------------------------------------+\n",
      "| Length of original list              :   4968 |\n",
      "| Number of unique tokens in original  :    269 |\n",
      "| Length of compressed list            :   4373 |\n",
      "| Number of unique tokens in compressed:    275 |\n",
      "| Final compression ratio              : 0.120  |\n",
      "| Length of vocabulary                 :    358 |\n",
      "| Length of vocabulary                 :    358 |\n",
      "| Total documents seen so far          :      0 |\n",
      "| Total words seen so far              :      0 |\n",
      "+-----------------------------------------------+\n",
      "Length with new encoder: 4373\n",
      "+-----------------------------------------------+\n",
      "|               Analysis Results                |\n",
      "+-----------------------------------------------+\n",
      "| Length of original list              :   4968 |\n",
      "| Number of unique tokens in original  :    269 |\n",
      "| Length of compressed list            :   1521 |\n",
      "| Number of unique tokens in compressed:    263 |\n",
      "| Final compression ratio              : 0.694  |\n",
      "| Length of vocabulary                 :   1731 |\n",
      "+-----------------------------------------------+\n",
      "Length with trained encoder: 1521\n"
     ]
    }
   ],
   "source": [
    "data_list = preprocess.get_meta_data(batch_size=1, data_type=\"test\", ignore_list=[\"case_id\", \"target\"], verbose= False)\n",
    "print(f\"Length of initial text: {len(data_list[0].text)}\")\n",
    "print(f\"Length with new encoder: {len(new_encoder.encode_text(data_list[0].text, verbose=True))}\")\n",
    "print(f\"Length with trained encoder: {len(trained_encoder.encode_text(data_list[0].text, verbose=True))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfca8fa-acff-44a5-b76e-e29bc5f3f7e5",
   "metadata": {},
   "source": [
    "As can be seen we able to compress the text by almost 70% with the trained version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91931e-d54e-4519-b317-38c26c3439cb",
   "metadata": {},
   "source": [
    "## How the vocabulary is created\n",
    "\n",
    "1. **Base Vocabulary from ASCII Characters**\n",
    "- **Rationale**: The ASCII character set includes standard letters, digits, and symbols, providing a comprehensive foundation.\n",
    "- **Implementation**: Start with all ASCII characters (values 32 to 126) to ensure basic textual elements are covered.\n",
    "\n",
    "2. **Handling Numbers, Dates, and Amounts**\n",
    "- **Rationale**: These formatted entities are crucial for maintaining contextual meaning.\n",
    "- **Implementation**: Extract and store numbers, dates, and amounts in a `MetaData` class during tokenization, preserving their whole units.\n",
    "\n",
    "3. **Training on All Possible Strings**\n",
    "- **Rationale**: A robust vocabulary should encompass all unique strings in the training data.\n",
    "- **Implementation**: Analyze and iteratively merge frequent pairs in the training set to build a comprehensive BPE vocabulary.\n",
    "\n",
    "4. **Specialized Tokens for Column Names**\n",
    "- **Rationale**: Column names play a unique role and are important for understanding tabular data.\n",
    "- **Implementation**: Add column names as unique tokens to directly recognize and preserve their significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3001370",
   "metadata": {},
   "source": [
    "### Future Considerations for BPE Vocabulary Creation\n",
    "- **Frequency Analysis**: Perform token frequency analysis to refine the vocabulary, ensuring it remains representative.\n",
    "- **Token Granularity**: Balance token granularity to capture nuances without overcomplicating the model.\n",
    "- **Context Preservation**: Maintain the data's context and meaning, especially for specialized entities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainstation_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
